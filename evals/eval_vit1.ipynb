{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227c3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c56bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"viskawei-johns-hopkins-university/vit-test/model-v0ib614v:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "516684a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_path = \"/Users/viskawei/Desktop/VIT/results/inspect/epoch=0-val_mae=0.5095_20250911_232705/batch_0/all.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe32aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.load(att_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c973993e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_idx', 'activations', 'logits', 'hidden_states', 'loss'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94a1ccf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['batch_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16b8c244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vit.embeddings.patch_embeddings.projection', 'vit.embeddings.patch_embeddings', 'vit.embeddings.dropout', 'vit.embeddings', 'vit.encoder.layer.0.layernorm_before', 'vit.encoder.layer.0.attention.attention.key', 'vit.encoder.layer.0.attention.attention.value', 'vit.encoder.layer.0.attention.attention.query', 'vit.encoder.layer.0.attention.attention', 'vit.encoder.layer.0.attention.output.dense', 'vit.encoder.layer.0.attention.output.dropout', 'vit.encoder.layer.0.attention.output', 'vit.encoder.layer.0.attention', 'vit.encoder.layer.0.layernorm_after', 'vit.encoder.layer.0.intermediate.dense', 'vit.encoder.layer.0.intermediate.intermediate_act_fn', 'vit.encoder.layer.0.intermediate', 'vit.encoder.layer.0.output.dense', 'vit.encoder.layer.0.output.dropout', 'vit.encoder.layer.0.output', 'vit.encoder.layer.0', 'vit.encoder.layer.1.layernorm_before', 'vit.encoder.layer.1.attention.attention.key', 'vit.encoder.layer.1.attention.attention.value', 'vit.encoder.layer.1.attention.attention.query', 'vit.encoder.layer.1.attention.attention', 'vit.encoder.layer.1.attention.output.dense', 'vit.encoder.layer.1.attention.output.dropout', 'vit.encoder.layer.1.attention.output', 'vit.encoder.layer.1.attention', 'vit.encoder.layer.1.layernorm_after', 'vit.encoder.layer.1.intermediate.dense', 'vit.encoder.layer.1.intermediate.intermediate_act_fn', 'vit.encoder.layer.1.intermediate', 'vit.encoder.layer.1.output.dense', 'vit.encoder.layer.1.output.dropout', 'vit.encoder.layer.1.output', 'vit.encoder.layer.1', 'vit.encoder', 'vit.layernorm', 'vit.pooler.dense', 'vit.pooler.activation', 'vit.pooler', 'vit', 'regressor', 'loss_fct'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['activations'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c82992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.patch_embeddings.projection torch.Size([100, 21, 4])\n",
      "vit.embeddings.patch_embeddings torch.Size([100, 21, 4])\n",
      "vit.embeddings.dropout torch.Size([100, 22, 4])\n",
      "vit.embeddings torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.layernorm_before torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention.attention.key torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention.attention.value torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention.attention.query torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention.attention_0 torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention.output.dense torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention.output.dropout torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention.output torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.attention torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.layernorm_after torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.intermediate.dense torch.Size([100, 22, 16])\n",
      "vit.encoder.layer.0.intermediate.intermediate_act_fn torch.Size([100, 22, 16])\n",
      "vit.encoder.layer.0.intermediate torch.Size([100, 22, 16])\n",
      "vit.encoder.layer.0.output.dense torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.output.dropout torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0.output torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.0 torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.layernorm_before torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention.attention.key torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention.attention.value torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention.attention.query torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention.attention_0 torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention.output.dense torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention.output.dropout torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention.output torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.attention torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.layernorm_after torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.intermediate.dense torch.Size([100, 22, 16])\n",
      "vit.encoder.layer.1.intermediate.intermediate_act_fn torch.Size([100, 22, 16])\n",
      "vit.encoder.layer.1.intermediate torch.Size([100, 22, 16])\n",
      "vit.encoder.layer.1.output.dense torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.output.dropout torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1.output torch.Size([100, 22, 4])\n",
      "vit.encoder.layer.1 torch.Size([100, 22, 4])\n",
      "vit.encoder_last_hidden_state torch.Size([100, 22, 4])\n",
      "vit.layernorm torch.Size([100, 22, 4])\n",
      "vit.pooler.dense torch.Size([100, 4])\n",
      "vit.pooler.activation torch.Size([100, 4])\n",
      "vit.pooler torch.Size([100, 4])\n",
      "vit_last_hidden_state torch.Size([100, 22, 4])\n",
      "vit_pooler_output torch.Size([100, 4])\n",
      "regressor torch.Size([100, 1])\n",
      "loss_fct torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for k, v in a['activations'].items():\n",
    "    if isinstance(v, list):\n",
    "        for i, vv in enumerate(v):\n",
    "            print(f\"{k}_{i}\", vv.shape)\n",
    "    elif isinstance(v, dict):\n",
    "        for kk, vv in v.items():\n",
    "            print(f\"{k}_{kk}\", vv.shape)\n",
    "    else:\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a77a278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1174, -0.0592, -0.1380,  0.1982],\n",
       "         [ 0.1169, -0.0570, -0.1364,  0.1966],\n",
       "         [ 0.1143, -0.0575, -0.1335,  0.1940],\n",
       "         ...,\n",
       "         [ 0.0856, -0.0371, -0.1097,  0.1480],\n",
       "         [ 0.0827, -0.0412, -0.1036,  0.1389],\n",
       "         [-0.0010, -0.0010,  0.0010,  0.0010]],\n",
       "\n",
       "        [[ 0.0939, -0.0549, -0.1160,  0.1677],\n",
       "         [ 0.1024, -0.0408, -0.1204,  0.1774],\n",
       "         [ 0.0962, -0.0581, -0.1175,  0.1775],\n",
       "         ...,\n",
       "         [ 0.0939, -0.0366, -0.1258,  0.1680],\n",
       "         [ 0.0985, -0.0507, -0.1129,  0.1650],\n",
       "         [-0.0010, -0.0010,  0.0010,  0.0010]],\n",
       "\n",
       "        [[ 0.1074, -0.0552, -0.1278,  0.1831],\n",
       "         [ 0.1089, -0.0526, -0.1278,  0.1846],\n",
       "         [ 0.1076, -0.0547, -0.1262,  0.1829],\n",
       "         ...,\n",
       "         [ 0.0924, -0.0382, -0.1149,  0.1618],\n",
       "         [ 0.0912, -0.0451, -0.1069,  0.1534],\n",
       "         [-0.0010, -0.0010,  0.0010,  0.0010]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0964, -0.0557, -0.1150,  0.1655],\n",
       "         [ 0.1063, -0.0458, -0.1246,  0.1784],\n",
       "         [ 0.1026, -0.0526, -0.1187,  0.1729],\n",
       "         ...,\n",
       "         [ 0.0929, -0.0382, -0.1228,  0.1647],\n",
       "         [ 0.0951, -0.0470, -0.1123,  0.1602],\n",
       "         [-0.0010, -0.0010,  0.0010,  0.0010]],\n",
       "\n",
       "        [[ 0.1147, -0.0587, -0.1358,  0.1945],\n",
       "         [ 0.1150, -0.0561, -0.1348,  0.1944],\n",
       "         [ 0.1128, -0.0569, -0.1320,  0.1916],\n",
       "         ...,\n",
       "         [ 0.0878, -0.0366, -0.1106,  0.1536],\n",
       "         [ 0.0866, -0.0427, -0.1019,  0.1453],\n",
       "         [-0.0010, -0.0010,  0.0010,  0.0010]],\n",
       "\n",
       "        [[ 0.1132, -0.0570, -0.1331,  0.1913],\n",
       "         [ 0.1130, -0.0554, -0.1320,  0.1904],\n",
       "         [ 0.1112, -0.0556, -0.1299,  0.1882],\n",
       "         ...,\n",
       "         [ 0.0897, -0.0388, -0.1093,  0.1550],\n",
       "         [ 0.0871, -0.0431, -0.1038,  0.1466],\n",
       "         [-0.0010, -0.0010,  0.0010,  0.0010]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"activations\"][\"vit.embeddings.patch_embeddings.projection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07715b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, yaml\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "sys.path.insert(0, '..')\n",
    "load_dotenv(find_dotenv(), override=False)  # 确保 .env 已加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0eca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598634ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_env(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return os.path.expanduser(os.path.expandvars(obj))\n",
    "    if isinstance(obj, list):\n",
    "        return [expand_env(x) for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: expand_env(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    cfg = expand_env(cfg)\n",
    "    return cfg\n",
    "\n",
    "config = load_config(os.path.join(os.path.expandvars(os.environ[\"CONFIG_DIR\"]),\"vit.yaml\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a76ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'warmup': {'global': False},\n",
       " 'model': {'name': 'vit',\n",
       "  'task_type': 'reg',\n",
       "  'num_labels': 1,\n",
       "  'image_size': 4096,\n",
       "  'patch_size': 200,\n",
       "  'hidden_size': 4,\n",
       "  'num_hidden_layers': 2,\n",
       "  'num_attention_heads': 1,\n",
       "  'stride_size': 200,\n",
       "  'proj_fn': 'SW'},\n",
       " 'train': {'batch_size': 128,\n",
       "  'ep': 1,\n",
       "  'debug': 0,\n",
       "  'workers': 10,\n",
       "  'save': True},\n",
       " 'loss': {'name': 'l1'},\n",
       " 'opt': {'type': 'adam',\n",
       "  'lr': 0.001,\n",
       "  'lr_sch': 'plateau',\n",
       "  'factor': 0.8,\n",
       "  'patience': 2},\n",
       " 'data': {'file_path': '/Users/viskawei/Desktop/VIT/data/test_1k/dataset.h5',\n",
       "  'val_path': '/Users/viskawei/Desktop/VIT/data/test_1k/dataset.h5',\n",
       "  'test_path': '/Users/viskawei/Desktop/VIT/data/test_1k/dataset.h5',\n",
       "  'num_samples': 100,\n",
       "  'num_test_samples': 100,\n",
       "  'param': 'T_eff',\n",
       "  'label_norm': 'minmax'},\n",
       " 'mask': {'mask_ratio': 0.85},\n",
       " 'noise': {'noise_level': 0},\n",
       " 'project': 'vit-test0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef3d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viskawei/Desktop/VIT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from src.utils import load_config\n",
    "from src.vit import ViTLModule, ViTDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc7317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model model with train loss\n",
      "Creating ViT_p200_h4_l2_a1_s200_pSW model with l1 loss\n",
      "Using RegSpec Dataset (regression)\n",
      "/Users/viskawei/Desktop/VIT/data/test_1k/dataset.h5 100 /Users/viskawei/Desktop/VIT/data/test_1k/dataset.h5 100 /Users/viskawei/Desktop/VIT/data/test_1k/dataset.h5 None ./results\n",
      "loading data from /Users/viskawei/Desktop/VIT/data/test_1k/dataset.h5 100\n",
      "torch.Size([100, 4096]) torch.Size([100, 4096]) torch.Size([4096]) 100 4096\n",
      "[test data] label normalization 'minmax': mean=None, std=None, min=[3750.0], max=[6000.0]\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = os.path.join(os.path.expandvars(os.environ[\"CKPT_DIR\"]),\"epoch=0-val_mae=0.5095.ckpt\")\n",
    "\n",
    "# Load Lightning module from checkpoint (rebuilds model from config)\n",
    "lm: ViTLModule = ViTLModule.load_from_checkpoint(ckpt_path, config=config)\n",
    "lm.eval()\n",
    "lm.to('cpu')\n",
    "\n",
    "# Data module and test loader\n",
    "dm = ViTDataModule.from_config(config, test_data=False)\n",
    "dm.setup(stage=\"test\")\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ed6bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_tensor_to_cpu(x: torch.Tensor) -> torch.Tensor:\n",
    "    try:\n",
    "        return x.detach().to(\"cpu\")\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def register_activation_hooks(model: torch.nn.Module, include_classes=None):\n",
    "    \"\"\"Register forward hooks for most leaf modules.\n",
    "\n",
    "    Returns: (activations: OrderedDict, handles: list)\n",
    "    \"\"\"\n",
    "    activations = OrderedDict()\n",
    "    handles = []\n",
    "\n",
    "    def should_hook(mod: torch.nn.Module) -> bool:\n",
    "        # Skip containers\n",
    "        from torch.nn.modules.container import Sequential, ModuleList, ModuleDict\n",
    "        if isinstance(mod, (Sequential, ModuleList, ModuleDict)):\n",
    "            return False\n",
    "        # Optionally filter by classes\n",
    "        if include_classes is not None and not isinstance(mod, include_classes):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook_fn(module, inputs, output):\n",
    "            try:\n",
    "                key = name\n",
    "                # Normalize output to tensors for saving\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    activations[key] = _safe_tensor_to_cpu(output)\n",
    "                elif isinstance(output, (list, tuple)):\n",
    "                    activations[key] = tuple(_safe_tensor_to_cpu(x) for x in output if isinstance(x, torch.Tensor))\n",
    "                elif isinstance(output, dict):\n",
    "                    out = {k: _safe_tensor_to_cpu(v) for k, v in output.items() if isinstance(v, torch.Tensor)}\n",
    "                    activations[key] = out\n",
    "                else:\n",
    "                    # Fallback: store repr\n",
    "                    activations[key] = repr(type(output))\n",
    "            except Exception as e:\n",
    "                activations[name] = {\"error\": repr(e)}\n",
    "        return hook_fn\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue  # skip root\n",
    "        if should_hook(module):\n",
    "            handles.append(module.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    return activations, handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576475a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts, handles = register_activation_hooks(lm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482369f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb09b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b59af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        for bi, batch in enumerate(test_loader):\n",
    "\n",
    "            flux, _, labels = batch\n",
    "            flux = flux.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Clear previous activations\n",
    "            acts.clear()\n",
    "\n",
    "            out = lm.model(\n",
    "                flux,\n",
    "                labels=labels,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c43b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 22, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['hidden_states'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "464b7916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['vit.embeddings.patch_embeddings.projection', 'vit.embeddings.patch_embeddings', 'vit.embeddings.dropout', 'vit.embeddings', 'vit.encoder.layer.0.layernorm_before', 'vit.encoder.layer.0.attention.attention.key', 'vit.encoder.layer.0.attention.attention.value', 'vit.encoder.layer.0.attention.attention.query', 'vit.encoder.layer.0.attention.attention', 'vit.encoder.layer.0.attention.output.dense', 'vit.encoder.layer.0.attention.output.dropout', 'vit.encoder.layer.0.attention.output', 'vit.encoder.layer.0.attention', 'vit.encoder.layer.0.layernorm_after', 'vit.encoder.layer.0.intermediate.dense', 'vit.encoder.layer.0.intermediate.intermediate_act_fn', 'vit.encoder.layer.0.intermediate', 'vit.encoder.layer.0.output.dense', 'vit.encoder.layer.0.output.dropout', 'vit.encoder.layer.0.output', 'vit.encoder.layer.0', 'vit.encoder.layer.1.layernorm_before', 'vit.encoder.layer.1.attention.attention.key', 'vit.encoder.layer.1.attention.attention.value', 'vit.encoder.layer.1.attention.attention.query', 'vit.encoder.layer.1.attention.attention', 'vit.encoder.layer.1.attention.output.dense', 'vit.encoder.layer.1.attention.output.dropout', 'vit.encoder.layer.1.attention.output', 'vit.encoder.layer.1.attention', 'vit.encoder.layer.1.layernorm_after', 'vit.encoder.layer.1.intermediate.dense', 'vit.encoder.layer.1.intermediate.intermediate_act_fn', 'vit.encoder.layer.1.intermediate', 'vit.encoder.layer.1.output.dense', 'vit.encoder.layer.1.output.dropout', 'vit.encoder.layer.1.output', 'vit.encoder.layer.1', 'vit.encoder', 'vit.layernorm', 'vit.pooler.dense', 'vit.pooler.activation', 'vit.pooler', 'vit', 'regressor', 'loss_fct'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabadaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def _safe_tensor_to_cpu(x: torch.Tensor) -> torch.Tensor:\n",
    "    try:\n",
    "        return x.detach().to(\"cpu\")\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "\n",
    "def collect_param_stats(model: torch.nn.Module) -> list[dict]:\n",
    "    stats = []\n",
    "    for name, p in model.named_parameters():\n",
    "        try:\n",
    "            cpu = p.detach().to(\"cpu\")\n",
    "            stats.append({\n",
    "                \"name\": name,\n",
    "                \"shape\": list(cpu.shape),\n",
    "                \"dtype\": str(cpu.dtype),\n",
    "                \"numel\": int(cpu.numel()),\n",
    "                \"mean\": float(cpu.mean().item()) if cpu.numel() > 0 else 0.0,\n",
    "                \"std\": float(cpu.std().item()) if cpu.numel() > 1 else 0.0,\n",
    "                \"min\": float(cpu.min().item()) if cpu.numel() > 0 else 0.0,\n",
    "                \"max\": float(cpu.max().item()) if cpu.numel() > 0 else 0.0,\n",
    "                \"norm\": float(cpu.norm().item()) if cpu.numel() > 0 else 0.0,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            stats.append({\"name\": name, \"error\": repr(e)})\n",
    "    return stats\n",
    "\n",
    "\n",
    "def register_activation_hooks(model: torch.nn.Module, include_classes=None):\n",
    "    \"\"\"Register forward hooks for most leaf modules.\n",
    "\n",
    "    Returns: (activations: OrderedDict, handles: list)\n",
    "    \"\"\"\n",
    "    activations = OrderedDict()\n",
    "    handles = []\n",
    "\n",
    "    def should_hook(mod: torch.nn.Module) -> bool:\n",
    "        # Skip containers\n",
    "        from torch.nn.modules.container import Sequential, ModuleList, ModuleDict\n",
    "        if isinstance(mod, (Sequential, ModuleList, ModuleDict)):\n",
    "            return False\n",
    "        # Optionally filter by classes\n",
    "        if include_classes is not None and not isinstance(mod, include_classes):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook_fn(module, inputs, output):\n",
    "            try:\n",
    "                key = name\n",
    "                # Normalize output to tensors for saving\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    activations[key] = _safe_tensor_to_cpu(output)\n",
    "                elif isinstance(output, (list, tuple)):\n",
    "                    activations[key] = tuple(_safe_tensor_to_cpu(x) for x in output if isinstance(x, torch.Tensor))\n",
    "                elif isinstance(output, dict):\n",
    "                    out = {k: _safe_tensor_to_cpu(v) for k, v in output.items() if isinstance(v, torch.Tensor)}\n",
    "                    activations[key] = out\n",
    "                else:\n",
    "                    # Fallback: store repr\n",
    "                    activations[key] = repr(type(output))\n",
    "            except Exception as e:\n",
    "                activations[name] = {\"error\": repr(e)}\n",
    "        return hook_fn\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue  # skip root\n",
    "        if should_hook(module):\n",
    "            handles.append(module.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    return activations, handles\n",
    "\n",
    "\n",
    "def save_activations(acts: OrderedDict, out_dir: str, batch_idx: int):\n",
    "    bdir = os.path.join(out_dir, f\"batch_{batch_idx}\")\n",
    "    os.makedirs(bdir, exist_ok=True)\n",
    "    meta = {}\n",
    "    for k, v in acts.items():\n",
    "        try:\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                path = os.path.join(bdir, f\"{k.replace('.', '_')}.pt\")\n",
    "                torch.save(v, path)\n",
    "                meta[k] = {\"type\": \"tensor\", \"shape\": list(v.shape), \"dtype\": str(v.dtype), \"file\": path}\n",
    "            elif isinstance(v, tuple):\n",
    "                entry = []\n",
    "                for i, t in enumerate(v):\n",
    "                    path = os.path.join(bdir, f\"{k.replace('.', '_')}_{i}.pt\")\n",
    "                    torch.save(t, path)\n",
    "                    entry.append({\"shape\": list(t.shape), \"dtype\": str(t.dtype), \"file\": path})\n",
    "                meta[k] = {\"type\": \"tuple\", \"items\": entry}\n",
    "            elif isinstance(v, dict):\n",
    "                entry = {}\n",
    "                for dk, t in v.items():\n",
    "                    path = os.path.join(bdir, f\"{k.replace('.', '_')}_{dk}.pt\")\n",
    "                    torch.save(t, path)\n",
    "                    entry[dk] = {\"shape\": list(t.shape), \"dtype\": str(t.dtype), \"file\": path}\n",
    "                meta[k] = {\"type\": \"dict\", \"items\": entry}\n",
    "            else:\n",
    "                meta[k] = {\"type\": \"other\", \"repr\": str(v)}\n",
    "        except Exception as e:\n",
    "            meta[k] = {\"error\": repr(e)}\n",
    "\n",
    "    with open(os.path.join(bdir, \"activations_index.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Load from ckpt, dump params and activations\")\n",
    "    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config\")\n",
    "    parser.add_argument(\"--ckpt\", type=str, required=True, help=\"Path to Lightning .ckpt file\")\n",
    "    parser.add_argument(\"--out\", type=str, default=None, help=\"Output dir (default results/inspect/<ckpt_basename>\")\n",
    "    parser.add_argument(\"--max-batches\", type=int, default=1, help=\"Number of test batches to capture\")\n",
    "    parser.add_argument(\"--save-full-param-tensors\", action=\"store_true\", help=\"Also save full parameter tensors to disk (can be large)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = _device()\n",
    "\n",
    "    config = load_config(args.config)\n",
    "\n",
    "    # Load Lightning module from checkpoint (rebuilds model from config)\n",
    "    lm: ViTLModule = ViTLModule.load_from_checkpoint(args.ckpt, config=config)\n",
    "    lm.eval()\n",
    "    lm.to(device)\n",
    "\n",
    "    # Data module and test loader\n",
    "    dm = ViTDataModule.from_config(config, test_data=False)\n",
    "    dm.setup(stage=\"test\")\n",
    "    test_loader = dm.test_dataloader()\n",
    "\n",
    "    # Output directory\n",
    "    ckpt_base = os.path.splitext(os.path.basename(args.ckpt))[0]\n",
    "    out_dir = args.out or os.path.join(\"results\", \"inspect\", f\"{ckpt_base}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save parameter stats (JSON) and optionally full tensors\n",
    "    os.makedirs(os.path.join(out_dir, \"params\"), exist_ok=True)\n",
    "    param_stats = collect_param_stats(lm.model)\n",
    "    with open(os.path.join(out_dir, \"params\", \"stats.json\"), \"w\") as f:\n",
    "        json.dump(param_stats, f, indent=2)\n",
    "\n",
    "    if args.save_full_param_tensors:\n",
    "        for name, p in lm.model.named_parameters():\n",
    "            try:\n",
    "                path = os.path.join(out_dir, \"params\", f\"{name.replace('.', '_')}.pt\")\n",
    "                torch.save(p.detach().to(\"cpu\"), path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Register hooks on the underlying model to capture activations\n",
    "    acts, handles = register_activation_hooks(lm.model)\n",
    "\n",
    "    # Run on test batches and save activations + model outputs\n",
    "    with torch.no_grad():\n",
    "        for bi, batch in enumerate(test_loader):\n",
    "            if bi >= args.max_batches:\n",
    "                break\n",
    "            flux, _, labels = batch\n",
    "            flux = flux.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Clear previous activations\n",
    "            acts.clear()\n",
    "\n",
    "            out = lm.model(\n",
    "                flux,\n",
    "                labels=labels,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            # Save activations captured by hooks\n",
    "            save_activations(acts, out_dir, bi)\n",
    "\n",
    "            # Save model-level outputs (logits, hidden_states, attentions)\n",
    "            bdir = os.path.join(out_dir, f\"batch_{bi}\")\n",
    "            if hasattr(out, \"logits\") and isinstance(out.logits, torch.Tensor):\n",
    "                torch.save(_safe_tensor_to_cpu(out.logits), os.path.join(bdir, \"logits.pt\"))\n",
    "            if hasattr(out, \"loss\") and out.loss is not None:\n",
    "                with open(os.path.join(bdir, \"loss.txt\"), \"w\") as f:\n",
    "                    f.write(str(float(out.loss.detach().cpu().item())))\n",
    "            try:\n",
    "                if getattr(out, \"hidden_states\", None) is not None:\n",
    "                    hs_dir = os.path.join(bdir, \"hidden_states\")\n",
    "                    os.makedirs(hs_dir, exist_ok=True)\n",
    "                    for li, h in enumerate(out.hidden_states):\n",
    "                        torch.save(_safe_tensor_to_cpu(h), os.path.join(hs_dir, f\"layer_{li}.pt\"))\n",
    "                if getattr(out, \"attentions\", None) is not None:\n",
    "                    attn_dir = os.path.join(bdir, \"attentions\")\n",
    "                    os.makedirs(attn_dir, exist_ok=True)\n",
    "                    for li, a in enumerate(out.attentions):\n",
    "                        torch.save(_safe_tensor_to_cpu(a), os.path.join(attn_dir, f\"layer_{li}.pt\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    print(f\"Inspection artifacts saved to: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81838d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --config CONFIG --ckpt CKPT [--out OUT]\n",
      "                             [--max-batches MAX_BATCHES]\n",
      "                             [--save-full-param-tensors]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --config, --ckpt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viskawei/Desktop/VIT/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26f62ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viskawei/Desktop/VIT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.basemodule import BaseModel, BaseLightningModule, BaseSpecDataset, BaseDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0f6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config  = load_config('../configs/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d737fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'image_size': 4096,\n",
       "  'patch_size': 100,\n",
       "  'hidden_size': 128,\n",
       "  'num_hidden_layers': 6,\n",
       "  'num_attention_heads': 8,\n",
       "  'num_labels': 2,\n",
       "  'stride_ratio': 1,\n",
       "  'proj_fn': 'SW'},\n",
       " 'train': {'batch_size': 128, 'ep': 1, 'debug': 0, 'workers': 24},\n",
       " 'loss': {'name': 'T1'},\n",
       " 'opt': {'type': 'adam',\n",
       "  'lr': 0.001,\n",
       "  'lr_sch': 'plateau',\n",
       "  'factor': 0.8,\n",
       "  'patience': 2},\n",
       " 'data': {'file_path': '/datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5',\n",
       "  'val_path': '/datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5',\n",
       "  'test_path': '/datascope/subaru/user/swei20/data/bosz50000/mag215/val_1k/dataset.h5',\n",
       "  'num_samples': 1000,\n",
       "  'num_test_samples': 1000,\n",
       "  'param_idx': 1},\n",
       " 'mask': {'mask_ratio': 0.85},\n",
       " 'noise': {'noise_level': 0},\n",
       " 'project': 'vit-test'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded988a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad4a06",
   "metadata": {},
   "source": [
    "from src.model import MyViT, runs perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1662643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96f7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(config, num_classes=2):\n",
    "    \"\"\"\n",
    "    Create a ViTConfig object based on the provided configuration.\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing model parameters.\n",
    "        num_classes (int): Number of output classes for classification tasks.\n",
    "        image_size (int): Size of the input images.\n",
    "    Returns:\n",
    "        ViTConfig: Config object for the Vision Transformer model.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    vit_config = ViTConfig(\n",
    "        image_size=config['model']['image_size'],\n",
    "        patch_size=config['model']['patch_size'],\n",
    "        num_channels=1,\n",
    "        hidden_size=config['model']['hidden_size'],\n",
    "        num_hidden_layers=config['model']['num_hidden_layers'],\n",
    "        num_attention_heads=config['model']['num_attention_heads'],\n",
    "        intermediate_size=4 * config['model']['hidden_size'],\n",
    "        stride_ratio=config['model']['stride_ratio'],\n",
    "        proj_fn=config['model']['proj_fn'],\n",
    "\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        is_encoder_decoder=False,\n",
    "        use_mask_token=False,\n",
    "        qkv_bias=True,\n",
    "        num_labels=num_classes,\n",
    "        noise_level=config['noise']['noise_level'],\n",
    "        learning_rate=config['opt']['lr'],\n",
    "    )\n",
    "    return vit_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26f5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_config = get_model_config(config, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b20845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import MyViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53847ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MyViT(vit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a810dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(2, 4096)\n",
    "label = torch.randint(0, 2, (2,))\n",
    "out = m(a, labels = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48bf4fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6387, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e8c1aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(0.6387, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0029,  0.0407],\n",
       "        [-0.1544,  0.0349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9c86c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03452424",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_PATH = '/datascope/subaru/user/swei20/model/bosz50000_mask.npy'\n",
    "import numpy as np\n",
    "\n",
    "#region --DATA-----------------------------------------------------------\n",
    "class SpecTrainDataset(BaseSpecDataset):\n",
    "    def load_data(self, stage=None) -> None:\n",
    "        super().load_data(stage=stage)\n",
    "        if self.mask_ratio is not None:\n",
    "            if self.mask_ratio < 1:\n",
    "                self.mask = np.load(MASK_PATH)\n",
    "                self.apply_mask()\n",
    "    \n",
    "class SpecTestDataset(BaseSpecDataset):\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset, stage='test'):\n",
    "        keys = ['file_path', 'val_path', 'test_path', 'num_samples', 'num_test_samples', 'root_dir', 'mask_ratio', 'mask_filler', 'mask', 'lvrg_num', 'lvrg_mask', 'noise_level', 'noise_max']\n",
    "        c = cls(**{k: getattr(dataset, k) for k in keys}) \n",
    "        if stage == 'val': c.num_test_samples = min(c.num_test_samples, 1000) \n",
    "        return c\n",
    "    def load_data(self, stage=None) -> None:\n",
    "        super().load_data(stage=stage)\n",
    "        if self.mask is None and self.mask_ratio is not None:\n",
    "            if self.mask_ratio < 1:\n",
    "                self.mask = np.load(MASK_PATH)\n",
    "            # self.mask = self.create_quantile_mask(self.error, ratio=self.mask_ratio)\n",
    "        if self.mask is not None: \n",
    "            self.mask_plot = {'wave': self.wave, 'error':self.error[0], 'mask': self.mask}\n",
    "            self.apply_mask()\n",
    "            self.mask_plot.update({'masked_error': self.error[0]})       \n",
    "        self.set_noise()    \n",
    "        \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.noisy[idx], self.flux[idx], self.error[idx]\n",
    "    \n",
    "    def set_noise(self, seed=42):\n",
    "        torch.manual_seed(seed)\n",
    "        self.noise = torch.randn_like(self.flux) * self.error * self.noise_level\n",
    "        self.noisy = self.flux + self.noise\n",
    "        self.flux_rms = torch.norm(self.flux, dim=-1)\n",
    "        self.snr0 = torch.div(self.flux_rms , torch.norm(self.noise, dim=-1))\n",
    "        \n",
    "    # def get_single_spectrum_noise_testset(self, sample_idx=0, repeat=1000, seed=42):\n",
    "    #     flux_0, error_0  = self.flux[sample_idx], self.error[sample_idx]\n",
    "    #     test_dataset = SingleSpectrumNoiseDataset(flux_0, error_0, noise_level=self.noise_level,repeat=repeat, seed=seed)\n",
    "    #     return test_dataset\n",
    "    \n",
    "#endregion --DATA-----------------------------------------------------------\n",
    "#region --DATAMODULE-----------------------------------------------------------\n",
    "class SpecDataModule(BaseDataModule):\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return super().from_config(dataset_cls=SpecTrainDataset, config=config)\n",
    "    def setup_test_dataset(self, stage):\n",
    "        if hasattr(self, 'train'):\n",
    "            return SpecTestDataset.from_dataset(self.train, stage) \n",
    "        return SpecTestDataset.from_config(self.config)\n",
    "#endregion --DATAMODULE-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d26b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassSpecDataset(BaseSpecDataset):\n",
    "    def __init__(self, param_idx=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.param_idx = param_idx  # 指定使用哪个参数作为标签\n",
    "        \n",
    "    def load_data(self, stage=None):\n",
    "        super().load_data(stage)\n",
    "        self.load_params(stage)\n",
    "        self.labels = (torch.tensor(self.logg > 2.5)).long() \n",
    "        \n",
    "        # 将连续参数离散化为分类标签\n",
    "        # params = torch.tensor(getattr(self, ['teff', 'logg', 'mh'][self.param_idx]))\n",
    "        # self.labels = self.discretize_params(params)\n",
    "        \n",
    "    def discretize_params(self, params, bins=10):\n",
    "        # 等频分箱创建分类标签\n",
    "        quantiles = torch.linspace(0, 1, bins+1)\n",
    "        bin_edges = torch.quantile(params, quantiles)\n",
    "        return torch.bucketize(params, bin_edges[1:-1]).long()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        flux, error = super().__getitem__(idx)\n",
    "        return flux, error, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4430ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/val_1k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5 None ./results\n"
     ]
    }
   ],
   "source": [
    "c = ClassSpecDataset.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8aa929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from /datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000\n",
      "torch.Size([1000, 4096]) torch.Size([1000, 4096]) torch.Size([4096]) 1000 4096\n"
     ]
    }
   ],
   "source": [
    "c.load_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba0d1660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5135, 0.6785, 0.5748,  ..., 0.4817, 0.4453, 0.4898]),\n",
       " tensor([0.0587, 0.0587, 0.0572,  ..., 0.0632, 0.0564, 0.0564]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96e9351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTDataModule(BaseDataModule):\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return super().from_config(dataset_cls=ClassSpecDataset, config=config)\n",
    "    def setup_test_dataset(self, stage):\n",
    "        return ClassSpecDataset.from_config(self.config)\n",
    "    # def setup(self, stage=None):\n",
    "    #     super().setup(stage)\n",
    "    #     # 添加通道维度 (B, L) -> (B, 1, L)\n",
    "    #     self.train.flux = self.train.flux.unsqueeze(1)\n",
    "    #     self.train.error = self.train.error.unsqueeze(1)\n",
    "    #     self.val.flux = self.val.flux.unsqueeze(1)\n",
    "    #     self.val.error = self.val.error.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ec4d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/val_1k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5 None ./results\n",
      "loading data from /datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000\n",
      "torch.Size([1000, 4096]) torch.Size([1000, 4096]) torch.Size([4096]) 1000 4096\n",
      "/datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/val_1k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5 None ./results\n",
      "loading data from /datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5 1000\n",
      "torch.Size([1000, 4096]) torch.Size([1000, 4096]) torch.Size([4096]) 1000 4096\n"
     ]
    }
   ],
   "source": [
    "dd = ViTDataModule.from_config(config)\n",
    "dd.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f49c572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ClassSpecDataset at 0x7f945ad64190>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c50444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.3198, 0.2749, 0.3066,  ..., 0.5203, 0.5208, 0.5208],\n",
      "        [0.5668, 0.5674, 0.5675,  ..., 0.4417, 0.4414, 0.4416],\n",
      "        [0.6054, 0.6077, 0.6191,  ..., 0.4398, 0.4393, 0.4391],\n",
      "        ...,\n",
      "        [0.5923, 0.5635, 0.5696,  ..., 0.4658, 0.4704, 0.4721],\n",
      "        [0.6074, 0.6073, 0.6073,  ..., 0.4152, 0.4156, 0.4155],\n",
      "        [0.1181, 0.1411, 0.1168,  ..., 0.7641, 0.8004, 0.7934]]), tensor([[0.1139, 0.1139, 0.1131,  ..., 0.1299, 0.1167, 0.1167],\n",
      "        [0.1397, 0.1397, 0.1384,  ..., 0.1555, 0.1389, 0.1389],\n",
      "        [0.0603, 0.0603, 0.0599,  ..., 0.0679, 0.0598, 0.0598],\n",
      "        ...,\n",
      "        [0.0399, 0.0399, 0.0397,  ..., 0.0443, 0.0395, 0.0395],\n",
      "        [0.1311, 0.1311, 0.1298,  ..., 0.1475, 0.1309, 0.1309],\n",
      "        [0.0731, 0.0731, 0.0721,  ..., 0.0891, 0.0801, 0.0801]]), tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6826)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "d = DataLoader(c, batch_size=100, num_workers=0, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    for batch in d:\n",
    "        print(batch)\n",
    "        flux, error, labels = batch\n",
    "        flux = flux.to(m.device)\n",
    "        error = error.to(m.device)\n",
    "        labels = labels.to(m.device)\n",
    "\n",
    "        output = m(flux, labels=labels)\n",
    "        \n",
    "        break\n",
    "output.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a087d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acc =Accuracy(task='multiclass', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "462c9f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5600)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acc(output.logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff5ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.basemodule import BaseTrainer\n",
    "t = BaseTrainer(config, num_gpus=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420497d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.basemodule import BaseLightningModule\n",
    "lm = BaseLightningModule.from_config(config, model_cls=MyViT, dataset_cls=ClassSpecDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6e623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "class ViTLModule(BaseLightningModule):\n",
    "    def __init__(self, model, data_module, config):\n",
    "        super().__init__(model=model, data_module=data_module, config=config)\n",
    "        self.save_hyperparameters()\n",
    "        self.loss_name = 'train'  # Set the loss name for logging\n",
    "        self.model.loss_name = self.loss_name  # Ensure the model has the loss name set\n",
    "        self.get_accuracy = Accuracy(task='multiclass', num_classes=config['num_labels'])\n",
    "\n",
    "    def forward(self, flux, labels, loss_only=True):\n",
    "        outputs = self.model(flux, labels=labels)\n",
    "        if loss_only:\n",
    "            return outputs.loss\n",
    "        else:\n",
    "            return outputs\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        flux, _, labels = batch\n",
    "        loss = self.forward(flux, labels, loss_only=True)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        flux, _, labels = batch\n",
    "        outputs = self.forward(flux, labels, loss_only=False)\n",
    "        self.log('val_loss', outputs.loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        accuracy = self.get_accuracy(outputs.logits, labels)\n",
    "        self.log('val_acc', accuracy, on_step=False, on_epoch=True,  prog_bar=False)\n",
    "        return outputs.loss\n",
    "        \n",
    "lm = ViTLModule(model=m, data_module=dd, config=config)\n",
    "t.fit(lm,datamodule=lm.data_module) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58ab58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.loss_name = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab0b98b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/local/tmp/swei20/miniconda3/envs/viska-torch-3/lib/python3.13/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /srv/local/tmp/swei20/miniconda3/envs/viska-torch-3/ ...\n",
      "/srv/local/tmp/swei20/miniconda3/envs/viska-torch-3/lib/python3.13/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/val_1k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5 None ./results\n",
      "loading data from /datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000\n",
      "torch.Size([1000, 4096]) torch.Size([1000, 4096]) torch.Size([4096]) 1000 4096\n",
      "/datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/val_1k/dataset.h5 1000 /datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5 None ./results\n",
      "loading data from /datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/local/tmp/swei20/miniconda3/envs/viska-torch-3/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/swei20/VIT/evals/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,5,6,7]\n",
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | MyViT | 1.2 M  | train\n",
      "----------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.834     Total estimated model params size (MB)\n",
      "112       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 4096]) torch.Size([1000, 4096]) torch.Size([4096]) 1000 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "lm = ViTLModule(model=m, data_module=dd, config=config)\n",
    "t.fit(lm,datamodule=lm.data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec92295",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {'file_path': '/home/swei20/SirenSpec/tests/spec/test_dataset.h5', 'num_samples': 10},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15ee2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31f9f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, config, use_wandb=False, num_gpus=None, sweep=False, ckpt_path=None):\n",
    "        # 创建数据模块\n",
    "        dm = ViTDataModule.from_config(config)\n",
    "        dm.setup()\n",
    "        \n",
    "        # 创建模型\n",
    "        model = ViTModel(config, num_labels=config.get('num_classes', 10))\n",
    "        \n",
    "        # 创建Lightning模块\n",
    "        self.lightning_module = ViTLightningModule(model, dm, config)\n",
    "        \n",
    "        # 其余初始化保持不变...\n",
    "        self.lightning_module.sweep = sweep\n",
    "        if use_wandb:\n",
    "            logger = L.pytorch.loggers.WandbLogger(\n",
    "                project=config['project'],\n",
    "                config=config,\n",
    "                name=config.get('exp_name', 'ViT_experiment'),\n",
    "                log_model=True\n",
    "            )\n",
    "        else:\n",
    "            logger = None\n",
    "            \n",
    "        self.t = SpecTrainer(config=config, logger=logger, \n",
    "                            num_gpus=num_gpus, sweep=sweep)\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "    # run方法保持不变..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f5129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
