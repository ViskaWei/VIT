{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227c3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26f62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.basemodule import BaseModel, BaseLightningModule, BaseSpecDataset, BaseDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8be4e",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0f6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config  = load_config('../configs/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d737fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'image_size': 4096,\n",
       "  'patch_size': 100,\n",
       "  'hidden_size': 128,\n",
       "  'num_hidden_layers': 6,\n",
       "  'num_attention_heads': 8,\n",
       "  'num_labels': 2,\n",
       "  'stride_ratio': 1,\n",
       "  'proj_fn': 'SW'},\n",
       " 'train': {'batch_size': 128, 'ep': 1, 'debug': 0, 'workers': 24},\n",
       " 'loss': {'name': 'T1'},\n",
       " 'opt': {'type': 'adam',\n",
       "  'lr': 0.001,\n",
       "  'lr_sch': 'plateau',\n",
       "  'factor': 0.8,\n",
       "  'patience': 2},\n",
       " 'data': {'file_path': '/datascope/subaru/user/swei20/data/bosz50000/test/mag215/train_100k/dataset.h5',\n",
       "  'val_path': '/datascope/subaru/user/swei20/data/bosz50000/mag215/train_1k/dataset.h5',\n",
       "  'test_path': '/datascope/subaru/user/swei20/data/bosz50000/mag215/val_1k/dataset.h5',\n",
       "  'num_samples': 1000,\n",
       "  'num_test_samples': 1000,\n",
       "  'param_idx': 1},\n",
       " 'mask': {'mask_ratio': 0.85},\n",
       " 'noise': {'noise_level': 0},\n",
       " 'project': 'vit-test'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39877376",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (1493945288.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m'image_size': ,\u001b[39m\n                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'image_size': ,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded988a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f39d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8b9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config  = load_config('../configs/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1662643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b96f7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(config, num_classes=2):\n",
    "    \"\"\"\n",
    "    Create a ViTConfig object based on the provided configuration.\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing model parameters.\n",
    "        num_classes (int): Number of output classes for classification tasks.\n",
    "        image_size (int): Size of the input images.\n",
    "    Returns:\n",
    "        ViTConfig: Config object for the Vision Transformer model.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    vit_config = ViTConfig(\n",
    "        image_size=config['model']['image_size'],\n",
    "        patch_size=config['model']['patch_size'],\n",
    "        num_channels=1,\n",
    "        hidden_size=config['model']['hidden_size'],\n",
    "        num_hidden_layers=config['model']['num_hidden_layers'],\n",
    "        num_attention_heads=config['model']['num_attention_heads'],\n",
    "        intermediate_size=4 * config['model']['hidden_size'],\n",
    "        stride_ratio=config['model']['stride_ratio'],\n",
    "        proj_fn=config['model']['proj_fn'],\n",
    "\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        is_encoder_decoder=False,\n",
    "        use_mask_token=False,\n",
    "        qkv_bias=True,\n",
    "        num_labels=num_classes,\n",
    "        noise_level=config['noise']['noise_level'],\n",
    "        learning_rate=config['opt']['lr'],\n",
    "    )\n",
    "    return vit_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ff0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_config = get_model_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf6c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d138cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ViTModel(BaseModel):\n",
    "    def __init__(self, config, num_labels=None):\n",
    "        super().__init__(model_name='ViT', loss_name='cls')\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=4096 or config['image_size'],\n",
    "            patch_size=200 or config['patch_size'],\n",
    "            num_channels=1,\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config['num_hidden_layers'],\n",
    "            num_attention_heads=config['num_attention_heads'],\n",
    "            intermediate_size=4 * config['hidden_size'],\n",
    "            num_labels=num_labels or config.get('num_labels', 2)\n",
    "        )\n",
    "        self.vit = ViTModel(vit_config)\n",
    "        self.classifier = nn.Linear(vit_config.hidden_size, vit_config.num_labels)\n",
    "        \n",
    "    def forward(self, x, labels=None):\n",
    "        outputs = self.vit(x)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        return {'logits': logits, 'loss': self.compute_loss(logits, labels)}\n",
    "    \n",
    "    def compute_loss(self, logits, labels):\n",
    "        return nn.CrossEntropyLoss()(logits, labels) if labels is not None else None\n",
    "    \n",
    "    def log_outputs(self, outputs, log_fn=print, stage=''):\n",
    "        loss = outputs.get('loss')\n",
    "        if loss is not None:\n",
    "            log_fn(f'{stage}_{self.loss_name}_loss', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32538856",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ViTModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eaa7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03452424",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_PATH = '/datascope/subaru/user/swei20/model/bosz50000_mask.npy'\n",
    "import numpy as np\n",
    "\n",
    "#region --DATA-----------------------------------------------------------\n",
    "class SpecTrainDataset(BaseSpecDataset):\n",
    "    def load_data(self, stage=None) -> None:\n",
    "        super().load_data(stage=stage)\n",
    "        if self.mask_ratio is not None:\n",
    "            if self.mask_ratio < 1:\n",
    "                self.mask = np.load(MASK_PATH)\n",
    "                self.apply_mask()\n",
    "    \n",
    "class SpecTestDataset(BaseSpecDataset):\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset, stage='test'):\n",
    "        keys = ['file_path', 'val_path', 'test_path', 'num_samples', 'num_test_samples', 'root_dir', 'mask_ratio', 'mask_filler', 'mask', 'lvrg_num', 'lvrg_mask', 'noise_level', 'noise_max']\n",
    "        c = cls(**{k: getattr(dataset, k) for k in keys}) \n",
    "        if stage == 'val': c.num_test_samples = min(c.num_test_samples, 1000) \n",
    "        return c\n",
    "    def load_data(self, stage=None) -> None:\n",
    "        super().load_data(stage=stage)\n",
    "        if self.mask is None and self.mask_ratio is not None:\n",
    "            if self.mask_ratio < 1:\n",
    "                self.mask = np.load(MASK_PATH)\n",
    "            # self.mask = self.create_quantile_mask(self.error, ratio=self.mask_ratio)\n",
    "        if self.mask is not None: \n",
    "            self.mask_plot = {'wave': self.wave, 'error':self.error[0], 'mask': self.mask}\n",
    "            self.apply_mask()\n",
    "            self.mask_plot.update({'masked_error': self.error[0]})       \n",
    "        self.set_noise()    \n",
    "        \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.noisy[idx], self.flux[idx], self.error[idx]\n",
    "    \n",
    "    def set_noise(self, seed=42):\n",
    "        torch.manual_seed(seed)\n",
    "        self.noise = torch.randn_like(self.flux) * self.error * self.noise_level\n",
    "        self.noisy = self.flux + self.noise\n",
    "        self.flux_rms = torch.norm(self.flux, dim=-1)\n",
    "        self.snr0 = torch.div(self.flux_rms , torch.norm(self.noise, dim=-1))\n",
    "        \n",
    "    # def get_single_spectrum_noise_testset(self, sample_idx=0, repeat=1000, seed=42):\n",
    "    #     flux_0, error_0  = self.flux[sample_idx], self.error[sample_idx]\n",
    "    #     test_dataset = SingleSpectrumNoiseDataset(flux_0, error_0, noise_level=self.noise_level,repeat=repeat, seed=seed)\n",
    "    #     return test_dataset\n",
    "    \n",
    "#endregion --DATA-----------------------------------------------------------\n",
    "#region --DATAMODULE-----------------------------------------------------------\n",
    "class SpecDataModule(BaseDataModule):\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return super().from_config(dataset_cls=SpecTrainDataset, config=config)\n",
    "    def setup_test_dataset(self, stage):\n",
    "        if hasattr(self, 'train'):\n",
    "            return SpecTestDataset.from_dataset(self.train, stage) \n",
    "        return SpecTestDataset.from_config(self.config)\n",
    "#endregion --DATAMODULE-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d26b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassSpecDataset(BaseSpecDataset):\n",
    "    def __init__(self, param_idx=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.param_idx = param_idx  # 指定使用哪个参数作为标签\n",
    "        \n",
    "    def load_data(self, stage=None):\n",
    "        super().load_data(stage)\n",
    "        self.load_params(stage)\n",
    "        \n",
    "        # 将连续参数离散化为分类标签\n",
    "        params = torch.tensor(getattr(self, ['teff', 'logg', 'mh'][self.param_idx]))\n",
    "        self.labels = self.discretize_params(params)\n",
    "        \n",
    "    def discretize_params(self, params, bins=10):\n",
    "        # 等频分箱创建分类标签\n",
    "        quantiles = torch.linspace(0, 1, bins+1)\n",
    "        bin_edges = torch.quantile(params, quantiles)\n",
    "        return torch.bucketize(params, bin_edges[1:-1]).long()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        flux, error = super().__getitem__(idx)\n",
    "        return flux, error, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4430ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/swei20/SirenSpec/tests/spec/test_dataset.h5 10 None None None None ./results\n"
     ]
    }
   ],
   "source": [
    "c = ClassSpecDataset.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4588e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.load_data(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec92295",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {'file_path': '/home/swei20/SirenSpec/tests/spec/test_dataset.h5', 'num_samples': 10},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "914f7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViTLightningModule(BaseLightningModule):\n",
    "    def __init__(self, model, data_module, config):\n",
    "        super().__init__(model, data_module, config)\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=config['num_labels'])\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        flux, error, labels = batch\n",
    "        noise = torch.randn_like(flux) * error * self.config.get('noise_level', 0)\n",
    "        inputs = flux + noise\n",
    "        outputs = self.model(inputs, labels=labels)\n",
    "        self.log(f'train_{self.model.loss_name}_loss', outputs['loss'])\n",
    "        return outputs['loss']\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        flux, error, labels = batch\n",
    "        outputs = self.model(flux, labels=labels)\n",
    "        acc = self.accuracy(outputs['logits'], labels)\n",
    "        self.log('val_loss', outputs['loss'])\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return outputs\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        flux, error, labels = batch\n",
    "        outputs = self.model(flux, labels=labels)\n",
    "        acc = self.accuracy(outputs['logits'], labels)\n",
    "        self.log('test_acc', acc)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4a818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a328b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf145f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassSpecDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d15ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTDataModule(BaseDataModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(dataset_cls=ClassSpecDataset, **kwargs)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        super().setup(stage)\n",
    "        # 添加通道维度 (B, L) -> (B, 1, L)\n",
    "        self.train.flux = self.train.flux.unsqueeze(1)\n",
    "        self.train.error = self.train.error.unsqueeze(1)\n",
    "        self.val.flux = self.val.flux.unsqueeze(1)\n",
    "        self.val.error = self.val.error.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31f9f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, config, use_wandb=False, num_gpus=None, sweep=False, ckpt_path=None):\n",
    "        # 创建数据模块\n",
    "        dm = ViTDataModule.from_config(config)\n",
    "        dm.setup()\n",
    "        \n",
    "        # 创建模型\n",
    "        model = ViTModel(config, num_labels=config.get('num_classes', 10))\n",
    "        \n",
    "        # 创建Lightning模块\n",
    "        self.lightning_module = ViTLightningModule(model, dm, config)\n",
    "        \n",
    "        # 其余初始化保持不变...\n",
    "        self.lightning_module.sweep = sweep\n",
    "        if use_wandb:\n",
    "            logger = L.pytorch.loggers.WandbLogger(\n",
    "                project=config['project'],\n",
    "                config=config,\n",
    "                name=config.get('exp_name', 'ViT_experiment'),\n",
    "                log_model=True\n",
    "            )\n",
    "        else:\n",
    "            logger = None\n",
    "            \n",
    "        self.t = SpecTrainer(config=config, logger=logger, \n",
    "                            num_gpus=num_gpus, sweep=sweep)\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "    # run方法保持不变..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f5129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viska-torch-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
