#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ZCA/PCA Whitening Report Generator

Given a torch-saved file at --cov_path that includes:
  - 'cov'     : [D, D] covariance matrix (float tensor, CPU is fine)
  - 'eigvals' : [D] eigenvalues (optional but preferred)
  - 'eigvecs' : [D, D] eigenvectors (optional but preferred)

This script computes several whitening transforms P (square [D,D] matrices):
  1) identity
  2) diagonal whitening (per-feature standardization)
  3) ZCA full-rank with shrinkage
  4) ZCA low-rank + avg in the orthogonal complement
  5) ZCA low-rank + identity in the orthogonal complement
  6) ZCA low-rank + zero in the orthogonal complement  (pure projection)

It evaluates verification metrics for each P:
  - white_err_self:  || P^T C_hat P - I ||_F / ||I||_F         (global)
  - white_err_subspace (if low-rank):  in the r-dim signal subspace
  - cond_before:     condition number of C_hat (from eigenvalues)
  - cond_after_full: condition number of P^T C_hat P (global)
  - cond_after_subspace (if low-rank): condition number in the r-subspace
  - off_ratio(P):    ||P - diag(P)||_F / ||P||_F                (how non-diagonal P is)

It saves:
  - A single figure with a heatmap of each P (cropped to top-left viz_dim x viz_dim)
  - A CSV with all metrics
  - Torch files for each P

And prints an English recommendation in the figure and console.

Usage:
  python zca_report.py --cov_path /path/to/cov.pt --out_dir ./zca_report --rank auto

Author: generated by ChatGPT (GPT-5 Pro)
"""
import argparse
import os
import math
from typing import Dict, Any, Tuple, Optional

import torch
import numpy as np
import matplotlib.pyplot as plt

# -----------------------------
# Utilities
# -----------------------------

def to_cpu64(x: torch.Tensor) -> torch.Tensor:
    return x.detach().to(device='cpu', dtype=torch.float64)

def symmetrize(C: torch.Tensor) -> torch.Tensor:
    return 0.5 * (C + C.T)

def sort_eig(lam: torch.Tensor, V: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    idx = torch.argsort(lam, descending=True)
    return lam[idx], V[:, idx]

def cov_shrink_eigs(lam: torch.Tensor, gamma: float) -> torch.Tensor:
    if gamma <= 0.0:
        return lam
    avg = lam.mean()
    return (1.0 - gamma) * lam + gamma * avg

def rel_fro(A: torch.Tensor, B: torch.Tensor) -> float:
    return float(torch.linalg.norm(A - B, 'fro') / torch.linalg.norm(B, 'fro'))

def off_ratio(P: torch.Tensor) -> float:
    return float(torch.linalg.norm(P - torch.diag(torch.diag(P)), 'fro') / torch.linalg.norm(P, 'fro'))

def cond_from_eigs(lam: torch.Tensor, eps: float = 1e-18) -> float:
    lam = torch.clamp(lam, min=eps)
    return float(lam.max() / lam.min())

def choose_rank_by_energy(lam: torch.Tensor, target: float = 0.99, r_cap: int = 64) -> int:
    lam_pos = torch.clamp(lam, min=0.0)
    cumsum = torch.cumsum(lam_pos, dim=0)
    total = cumsum[-1].clamp(min=1e-30)
    frac = cumsum / total
    r = int(torch.searchsorted(frac, torch.tensor(target, dtype=frac.dtype)) + 1)
    r = max(1, min(r, r_cap, lam.numel()-1))
    return r

def subspace_white_err(P: torch.Tensor, C: torch.Tensor, Vr: torch.Tensor) -> float:
    """Check whitening error inside the r-dim subspace spanned by Vr."""
    Ihat = P.T @ C @ P
    Ih_sub = Vr.T @ Ihat @ Vr
    I_r = torch.eye(Vr.shape[1], dtype=Vr.dtype)
    return rel_fro(Ih_sub, I_r)

def subspace_cond(P: torch.Tensor, C: torch.Tensor, Vr: torch.Tensor, eps: float = 1e-18) -> float:
    Ihat = P.T @ C @ P
    Ih_sub = Vr.T @ Ihat @ Vr
    lam, _ = torch.linalg.eigh(symmetrize(Ih_sub))
    lam = torch.clamp(lam, min=eps)
    return float(lam.max() / lam.min())

# -----------------------------
# Variants of P
# -----------------------------

def P_identity(D: int) -> torch.Tensor:
    return torch.eye(D, dtype=torch.float64)

def P_diag_whiten(C: torch.Tensor, eps_eff: float) -> torch.Tensor:
    d = torch.diag(C).clamp(min=0.0)
    w = 1.0 / torch.sqrt(d + eps_eff)
    return torch.diag(w)

def P_zca_full(V: torch.Tensor, lam: torch.Tensor, eps_eff: float, gamma: float) -> Tuple[torch.Tensor, torch.Tensor]:
    lam_hat = cov_shrink_eigs(lam, gamma)
    inv_sqrt = torch.rsqrt(lam_hat + eps_eff)
    P = (V * inv_sqrt) @ V.T
    C_hat = V @ torch.diag(lam_hat) @ V.T
    return P, C_hat

def P_zca_lowrank_mode(V: torch.Tensor, lam: torch.Tensor, r: int, eps_eff: float, gamma: float,
                       mode: str = "avg", tail_stat: str = "median",
                       floor_rel: float = 1e-3) -> Tuple[torch.Tensor, torch.Tensor, float, float, torch.Tensor]:
    """
    Return:
      P:          [D,D] low-rank ZCA with chosen perp mode
      C_hat:      [D,D] shrinked covariance used for self-consistency checks
      lam0:       scalar tail representative
      s_perp:     scalar perp scaling
      Vr:         [D,r] signal subspace basis
    """
    lam_hat = cov_shrink_eigs(lam, gamma)
    D = lam_hat.numel()
    assert 1 <= r < D, "Need 1 <= r < D for low-rank"
    Vr = V[:, :r]
    proj = Vr @ Vr.T
    I = torch.eye(D, dtype=torch.float64)

    inv_sqrt_r = torch.rsqrt(lam_hat[:r] + eps_eff)

    # tail representative
    tail = lam_hat[r:]
    if tail.numel() == 0:
        lam0 = lam_hat[r-1]
    else:
        if tail_stat == "avg":
            lam0 = tail.mean()
        elif tail_stat == "median":
            lam0 = tail.median()
        else:  # trimmed mean
            k = int(0.2 * tail.numel())
            vals = torch.sort(tail).values
            lam0 = vals[k:-k].mean() if 2*k < tail.numel() else vals.mean()

    lam0_floor = floor_rel * lam_hat[:r].mean()
    lam0 = torch.clamp(lam0, min=lam0_floor)

    if mode == "avg":
        s_perp = 1.0 / torch.sqrt(lam0 + eps_eff)
        P = (Vr * inv_sqrt_r) @ Vr.T + s_perp * (I - proj)
    elif mode == "identity":
        s_perp = 1.0
        P = (Vr * inv_sqrt_r) @ Vr.T + (I - proj)
    elif mode == "zero":
        s_perp = 0.0
        P = (Vr * inv_sqrt_r) @ Vr.T
    else:
        raise ValueError(f"Unknown mode: {mode}")

    C_hat = V @ torch.diag(lam_hat) @ V.T
    return P, C_hat, float(lam0), float(s_perp), Vr

# -----------------------------
# Report / Visualization
# -----------------------------

def make_report(cov_path: str,
                out_dir: str,
                rank: str = "auto",
                energy_target: float = 0.99,
                r_cap: int = 64,
                gamma: float = 0.10,
                eps_rel: float = 1e-6,
                viz_dim: int = 256,
                tail_stat: str = "median",
                floor_rel: float = 1e-3) -> Dict[str, Any]:

    os.makedirs(out_dir, exist_ok=True)

    blob = torch.load(cov_path, map_location='cpu')
    # Retrieve C, V, lam
    if 'cov' in blob:
        C = to_cpu64(blob['cov'])
    else:
        C = None
    if 'eigvals' in blob and 'eigvecs' in blob:
        lam = to_cpu64(blob['eigvals']).flatten()
        V = to_cpu64(blob['eigvecs'])
        # ensure shapes
        assert V.shape[0] == V.shape[1] == lam.numel(), "eigvecs/eigvals shapes mismatch"
        # sort descending if needed
        lam, V = sort_eig(lam, V)
    else:
        assert C is not None, "Need 'cov' or ('eigvals','eigvecs') in the file."
        lam, V = torch.linalg.eigh(symmetrize(C))
        lam, V = sort_eig(lam, V)

    D = V.shape[0]
    if C is None:
        C = V @ torch.diag(lam) @ V.T
    else:
        C = to_cpu64(C)
        C = symmetrize(C)

    eps_eff = eps_rel * float(lam.max())

    # Rank selection
    if rank == "auto":
        r_auto = choose_rank_by_energy(lam, target=energy_target, r_cap=r_cap)
        r = r_auto
    else:
        r = int(rank)

    # Prepare variants
    variants = []

    # Identity
    P_id = P_identity(D)
    lam_hat_for_metrics = cov_shrink_eigs(lam, gamma)
    C_hat = V @ torch.diag(lam_hat_for_metrics) @ V.T
    Ih_id = symmetrize(P_id.T @ C_hat @ P_id)
    lam_Ih_id, _ = torch.linalg.eigh(Ih_id)
    metrics_id = {
        'name': 'identity',
        'white_err_self': rel_fro(Ih_id, torch.eye(D, dtype=torch.float64)),
        'white_err_subspace': None,
        'cond_before': cond_from_eigs(lam_hat_for_metrics),
        'cond_after_full': cond_from_eigs(lam_Ih_id),
        'cond_after_subspace': None,
        'off_ratio': off_ratio(P_id)
    }
    variants.append(('identity', P_id, metrics_id))

    # Diagonal whitening (per-feature)
    P_diag = P_diag_whiten(C_hat, eps_eff)
    Ih_diag = symmetrize(P_diag.T @ C_hat @ P_diag)
    lam_Ih_diag, _ = torch.linalg.eigh(Ih_diag)
    metrics_diag = {
        'name': 'diag_whiten',
        'white_err_self': rel_fro(Ih_diag, torch.eye(D, dtype=torch.float64)),
        'white_err_subspace': None,
        'cond_before': cond_from_eigs(lam_hat_for_metrics),
        'cond_after_full': cond_from_eigs(lam_Ih_diag),
        'cond_after_subspace': None,
        'off_ratio': off_ratio(P_diag)
    }
    variants.append(('diag_whiten', P_diag, metrics_diag))

    # ZCA full (with shrinkage)
    P_full, C_hat_full = P_zca_full(V, lam, eps_eff=eps_eff, gamma=gamma)
    Ih_full = symmetrize(P_full.T @ C_hat_full @ P_full)
    lam_Ih_full, _ = torch.linalg.eigh(Ih_full)
    metrics_full = {
        'name': f'zca_full(gamma={gamma:.2f})',
        'white_err_self': rel_fro(Ih_full, torch.eye(D, dtype=torch.float64)),
        'white_err_subspace': None,
        'cond_before': cond_from_eigs(cov_shrink_eigs(lam, gamma)),
        'cond_after_full': cond_from_eigs(lam_Ih_full),
        'cond_after_subspace': None,
        'off_ratio': off_ratio(P_full)
    }
    variants.append(('zca_full', P_full, metrics_full))

    # Low-rank ZCA + avg
    P_avg, C_hat_lr, lam0_avg, s_perp_avg, Vr = P_zca_lowrank_mode(V, lam, r=r, eps_eff=eps_eff, gamma=gamma,
                                                                    mode="avg", tail_stat=tail_stat, floor_rel=floor_rel)
    Ih_avg = symmetrize(P_avg.T @ C_hat_lr @ P_avg)
    lam_Ih_avg, _ = torch.linalg.eigh(Ih_avg)
    metrics_avg = {
        'name': f'zca_lowrank_avg(r={r}, gamma={gamma:.2f})',
        'white_err_self': rel_fro(Ih_avg, torch.eye(D, dtype=torch.float64)),
        'white_err_subspace': subspace_white_err(P_avg, C_hat_lr, Vr),
        'cond_before': cond_from_eigs(cov_shrink_eigs(lam, gamma)),
        'cond_after_full': cond_from_eigs(lam_Ih_avg),
        'cond_after_subspace': subspace_cond(P_avg, C_hat_lr, Vr),
        'off_ratio': off_ratio(P_avg)
    }
    variants.append(('zca_lowrank_avg', P_avg, metrics_avg))

    # Low-rank ZCA + identity
    P_idp, C_hat_lr2, _, _, Vr2 = P_zca_lowrank_mode(V, lam, r=r, eps_eff=eps_eff, gamma=gamma,
                                                     mode="identity", tail_stat=tail_stat, floor_rel=floor_rel)
    Ih_idp = symmetrize(P_idp.T @ C_hat_lr2 @ P_idp)
    lam_Ih_idp, _ = torch.linalg.eigh(Ih_idp)
    metrics_idp = {
        'name': f'zca_lowrank_identity(r={r}, gamma={gamma:.2f})',
        'white_err_self': rel_fro(Ih_idp, torch.eye(D, dtype=torch.float64)),
        'white_err_subspace': subspace_white_err(P_idp, C_hat_lr2, Vr2),
        'cond_before': cond_from_eigs(cov_shrink_eigs(lam, gamma)),
        'cond_after_full': cond_from_eigs(lam_Ih_idp),
        'cond_after_subspace': subspace_cond(P_idp, C_hat_lr2, Vr2),
        'off_ratio': off_ratio(P_idp)
    }
    variants.append(('zca_lowrank_identity', P_idp, metrics_idp))

    # Low-rank ZCA + zero (pure projection)
    P_zero, C_hat_lr3, _, _, Vr3 = P_zca_lowrank_mode(V, lam, r=r, eps_eff=eps_eff, gamma=gamma,
                                                      mode="zero", tail_stat=tail_stat, floor_rel=floor_rel)
    Ih_zero = symmetrize(P_zero.T @ C_hat_lr3 @ P_zero)
    lam_Ih_zero, _ = torch.linalg.eigh(Ih_zero)
    metrics_zero = {
        'name': f'zca_lowrank_zero(r={r}, gamma={gamma:.2f})',
        'white_err_self': rel_fro(Ih_zero, torch.eye(D, dtype=torch.float64)),  # expected large
        'white_err_subspace': subspace_white_err(P_zero, C_hat_lr3, Vr3),       # should be small
        'cond_before': cond_from_eigs(cov_shrink_eigs(lam, gamma)),
        'cond_after_full': cond_from_eigs(lam_Ih_zero),                         # expected large
        'cond_after_subspace': subspace_cond(P_zero, C_hat_lr3, Vr3),
        'off_ratio': off_ratio(P_zero)
    }
    variants.append(('zca_lowrank_zero', P_zero, metrics_zero))

    # -----------------------------
    # Save Ps and metrics
    # -----------------------------
    metrics_rows = []
    for key, P, m in variants:
        # Save P
        torch.save({'P': P}, os.path.join(out_dir, f'P_{key}.pt'))
        # Collect metrics
        row = {'variant': key}
        row.update(m)
        metrics_rows.append(row)

    # To avoid pandas dependency, write a simple CSV
    csv_path = os.path.join(out_dir, 'metrics.csv')
    # Collect headers
    headers = sorted({k for row in metrics_rows for k in row.keys()})
    with open(csv_path, 'w') as f:
        f.write(','.join(headers) + '\n')
        for row in metrics_rows:
            vals = [row.get(h, '') for h in headers]
            f.write(','.join(str(v) for v in vals) + '\n')

    # -----------------------------
    # Visualization (heatmaps + text report)
    # -----------------------------
    viz_dim = min(viz_dim, D)
    n = len(variants)
    cols = 3
    rows = math.ceil((n + 1) / cols)  # +1 for text panel
    fig = plt.figure(figsize=(4*cols, 4*rows), dpi=120)
    gs = fig.add_gridspec(rows, cols, wspace=0.3, hspace=0.4)

    # Heatmaps
    for i, (key, P, m) in enumerate(variants):
        r_i = i // cols
        c_i = i % cols
        ax = fig.add_subplot(gs[r_i, c_i])
        patch = P[:viz_dim, :viz_dim].numpy()
        # Use high-contrast colormap (RdBu_r for red-blue diverging, or 'seismic', 'coolwarm')
        im = ax.imshow(patch, aspect='auto', interpolation='nearest', cmap='RdBu_r')
        ax.set_title(f"{m['name']}\nwhite_err={m['white_err_self']:.3e}\ncond_after={m['cond_after_full']:.3e}", fontsize=9)
        ax.set_xlabel(f"P[0:{viz_dim},0:{viz_dim}]")
        ax.set_ylabel("")
        # Add colorbar for each subplot
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

    # Recommendation text (last panel)
    # Heuristic: if top-r covers >= energy_target and tail is tiny, recommend lowrank_avg; else zca_full.
    frac_energy_r = float(torch.cumsum(torch.clamp(lam, min=0.0), dim=0)[r-1] / torch.clamp(torch.sum(torch.clamp(lam, min=0.0)), min=1e-30))
    tail = lam[r:]
    tail_med = float(tail.median()) if tail.numel() > 0 else float(lam[max(r-1,0)])
    ratio_tail = tail_med / float(lam[0])
    if (frac_energy_r >= energy_target) and (ratio_tail < 1e-3):
        recommended = 'zca_lowrank_avg'
        reason = (f"Eigen-spectrum is strongly low-rank (top-{r} explains {frac_energy_r*100:.1f}% variance; "
                  f"tail median/lead â‰ˆ {ratio_tail:.2e}). "
                  "Low-rank ZCA + avg avoids inflating near-zero tail, preserves pixel-space geometry, "
                  "and yields near-identity whitening in the signal subspace with well-conditioned global scaling.")
    else:
        recommended = 'zca_full'
        reason = ("Spectrum is not extremely low-rank at chosen r, or tail not negligible; "
                  "full-rank ZCA with mild shrinkage best equalizes curvature across all directions "
                  "while keeping the signal geometry in pixel space.")

    # Compose metrics summary
    def find_metrics(name_key: str) -> Dict[str, Any]:
        for k, _, m in variants:
            if k == name_key:
                return m
        return {}

    m_rec = find_metrics(recommended)
    r_txt = (
        f"Recommended: {m_rec.get('name', recommended)}\n"
        f"  - white_err_self: {m_rec.get('white_err_self','n/a')}\n"
        f"  - cond_before:    {m_rec.get('cond_before','n/a')}\n"
        f"  - cond_after:     {m_rec.get('cond_after_full','n/a')}\n"
        f"Why:\n  {reason}\n"
        f"(rank selection: r={r}, energy_target={energy_target}, gamma={gamma}, eps_rel={eps_rel})"
    )

    ax_txt = fig.add_subplot(gs[(n) // cols, (n) % cols])
    ax_txt.axis('off')
    ax_txt.text(0.0, 1.0, r_txt, va='top', ha='left', fontsize=10, wrap=True)

    fig.suptitle("Whitening Matrices P (Croppped Heatmaps) and Metrics", fontsize=12)
    out_png = os.path.join(out_dir, "report.png")
    fig.savefig(out_png, bbox_inches='tight')
    plt.close(fig)

    # Console summary
    print("\n=== Whitening Report Summary ===")
    print(f"D = {D}, chosen rank r = {r} (energy@r = {frac_energy_r*100:.2f}%)")
    for _, _, m in variants:
        print(f"- {m['name']:>28s} | white_err={m['white_err_self']:.3e} | cond_after={m['cond_after_full']:.3e} | off_ratio={m['off_ratio']:.3f}")

    print(f"\nRecommended: {m_rec.get('name', recommended)}")
    print(reason)
    print(f"\nOutputs:\n  - Figure: {out_png}\n  - Metrics CSV: {csv_path}\n  - P matrices: P_*.pt in {out_dir}")

    return {
        'out_dir': out_dir,
        'figure': out_png,
        'csv': csv_path,
        'rank': r,
        'recommended': m_rec.get('name', recommended),
        'energy_at_r': frac_energy_r
    }

def main():
    parser = argparse.ArgumentParser(description="Generate a whitening report from precomputed covariance/eigendecomposition.")
    parser.add_argument("--cov_path", type=str, required=True, help="Path to torch file with 'cov', 'eigvals', 'eigvecs'.")
    parser.add_argument("--out_dir", type=str, default="./zca_report", help="Output directory.")
    parser.add_argument("--rank", type=str, default="auto", help="Rank for low-rank methods (int or 'auto').")
    parser.add_argument("--energy_target", type=float, default=0.99, help="Energy target for auto rank selection.")
    parser.add_argument("--r_cap", type=int, default=64, help="Max rank when auto-selecting.")
    parser.add_argument("--gamma", type=float, default=0.10, help="Shrinkage for eigenvalues (0..1).")
    parser.add_argument("--eps_rel", type=float, default=1e-6, help="Relative epsilon: eps = eps_rel * max(eigval).")
    parser.add_argument("--viz_dim", type=int, default=256, help="Crop size for P heatmaps (top-left block).")
    parser.add_argument("--tail_stat", type=str, default="median", choices=["median","avg","trimmed"],
                        help="Statistic for tail eigenvalue representative in low-rank+avg.")
    parser.add_argument("--floor_rel", type=float, default=1e-3,
                        help="Relative floor for tail eigenvalue representative (vs mean of top-r).")

    args = parser.parse_args()
    make_report(cov_path=args.cov_path,
                out_dir=args.out_dir,
                rank=args.rank,
                energy_target=args.energy_target,
                r_cap=args.r_cap,
                gamma=args.gamma,
                eps_rel=args.eps_rel,
                viz_dim=args.viz_dim,
                tail_stat=args.tail_stat,
                floor_rel=args.floor_rel)

if __name__ == "__main__":
    main()
# python scripts/prepro/zca_report.py --cov_path /Users/viskawei/Desktop/VIT/data/cov.pt --out_dir ./zca_report --rank auto --energy_target 0.99 --r_cap 64 --gamma 0.10 --eps_rel 1e-6 --viz_dim 256 --tail_stat median --floor_rel 1e-3
# python zca_report.py 
#   --cov_path /Users/viskawei/Desktop/VIT/data/pca/cov.pt\
#   --out_dir ./zca_report \
#   --rank auto \
#   --energy_target 0.99 \
#   --r_cap 64 \
#   --gamma 0.10 \
#   --eps_rel 1e-6 \
#   --viz_dim 256 \
#   --tail_stat median \
#   --floor_rel 1e-3
